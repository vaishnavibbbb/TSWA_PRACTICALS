{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"cell_execution_strategy":"setup"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## TSWA PRACTICAL 7 - Text Normalization"],"metadata":{"id":"yyl7rVhFIatK"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-62vtE2DIu6u","executionInfo":{"status":"ok","timestamp":1719502024525,"user_tz":-330,"elapsed":14866,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"d2300f70-830a-4f6a-8fba-1b1e88c78e28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"]}]},{"cell_type":"code","source":["import re\n","import string\n","import nltk"],"metadata":{"id":"df4Ks_cROvCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize , sent_tokenize\n","from nltk.corpus import wordnet"],"metadata":{"id":"VwdY7BhyO0Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"metadata":{"id":"tPrk2umGOk05","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719502222452,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sapna Girme","userId":"14803278953583791113"}},"outputId":"5b7f2415-0fca-44c4-ebb9-4419000401c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"lUfTOQD8PPnu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719502232692,"user_tz":-330,"elapsed":6828,"user":{"displayName":"Sapna Girme","userId":"14803278953583791113"}},"outputId":"cccb6247-5262-46ef-f91c-2456bb2eccd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqW_RlvuOPdr"},"outputs":[],"source":["import contractions"]},{"cell_type":"code","source":["def contraction_remover(text):\n","    expanded_words = []\n","    for word in text.split():\n","    # using contractions.fix to expand the shortened words\n","        expanded_words.append(contractions.fix(word))\n","    expanded_text = ' '.join(expanded_words)\n"],"metadata":{"id":"H6Ra0Dn8OX5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_corpus(df):\n","    # Remove HTML tags\n","    df['Preprocess_Article'] = df['Article'].apply(lambda x: re.sub(r'<.*?>', '', x))\n","\n","    # Convert to lowercase\n","    df['Preprocess_Article'] = df['Preprocess_Article'].str.lower()\n","\n","    # Remove URLs\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'http\\S+|www\\.\\S+', '', x))\n","\n","    # Remove email addresses\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n","\n","    # Remove phone numbers\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\d{10}', '', x))\n","\n","    # Handle negation\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\bnot\\b(\\w+)', r'not_\\1', x))\n","\n","    # Remove special characters and punctuation\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n","\n","    # Remove numeric tokens\n","    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\b\\d+\\b', '', x))\n","\n","    # Tokenization\n","    df['tokens'] = df['Preprocess_Article'].apply(word_tokenize)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in x])\n","\n","     # Convert tokens into a single string\n","    df['Clean Article'] = df['tokens'].apply(lambda x: ' '.join(x))\n","\n","    return df"],"metadata":{"id":"T9g_2zomPgcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function to map POS tag to WordNet POS tag\n","def get_wordnet_pos(word):\n","  tag = nltk.pos_tag([word])[0][1][0].upper()\n","  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n","  return tag_dict.get(tag, wordnet.NOUN)"],"metadata":{"id":"9F1nea9cZMn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups"],"metadata":{"id":"KVqOUNZEQSEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fetch data\n","\n","data = fetch_20newsgroups(subset='all')\n","data = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n","data_labels_map = dict(enumerate(data.target_names))"],"metadata":{"id":"Oe7EwsNDQA8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create objects for each package\n","import numpy as np\n","#import text_normalizer as tn\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"o9rUtI1CQemB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# building the dataframe for the data extracted from newgroups\n","# Create a corpus of newsgroup sentences and create the data frame\n","corpus=data.data\n","target_labels=data.target\n","target_names = [data_labels_map[label] for label in data.target]\n","data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n","print(data_df.shape)\n","data_df.head(10)"],"metadata":{"id":"B69zflU-QamV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.info()"],"metadata":{"id":"5YxplfbTyA__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalize_corpus(data_df)"],"metadata":{"id":"B89rX5QgSqor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df.info()"],"metadata":{"id":"VP0wnustzRHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# view sample data\n","data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n","data_df.head(10)"],"metadata":{"id":"pToY8qSW7t6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove any unwanted characters\n","data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n","data_df.info()"],"metadata":{"id":"dd_NUmze79F8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_df = data_df.dropna().reset_index(drop=True)\n","data_df.info()"],"metadata":{"id":"7JaFOQGs8KwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a csv file of the cleaned doucment so that it can be reused\n","data_df.to_csv('clean_newsgroups.csv', index=False)"],"metadata":{"id":"9LXddnh68RjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data - training and testing\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"zIReOtmt9B1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n","np.array(data_df['Target Name']),test_size=0.33, random_state=42)"],"metadata":{"id":"AfjJm8xW9GPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_corpus.shape, test_corpus.shape"],"metadata":{"id":"LV37Lv3l9Veu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the dictionary for train and test data\n","from collections import Counter"],"metadata":{"id":"zGSTA2EH9fTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trd = dict(Counter(train_label_names))\n","tsd = dict(Counter(test_label_names))"],"metadata":{"id":"pchrhgAX9ifP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trd"],"metadata":{"id":"CswxNlvAQ3qN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tsd"],"metadata":{"id":"N_Sa_2YURE1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd],columns=['Target Label', 'Train Count', 'Test Count'])\n",".sort_values(by=['Train Count', 'Test Count'],ascending=False))"],"metadata":{"id":"RzvlBk8V9mxL"},"execution_count":null,"outputs":[]}]}