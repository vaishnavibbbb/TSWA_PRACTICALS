{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## TSWA PRACTICAL 8 - Bag of Words , TF-IDF Models"],"metadata":{"id":"4t1WhVb8J65u"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyYByCOHKCt-","executionInfo":{"status":"ok","timestamp":1719499712775,"user_tz":-330,"elapsed":35331,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"5d05b6c8-853d-4579-9cad-8c65f1a25a0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2RLBbK0Fg95","executionInfo":{"status":"ok","timestamp":1719499731685,"user_tz":-330,"elapsed":18919,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"f3bd0333-35e1-4352-89c4-937af74a2063"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"]}]},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16piYI4cVuuS","executionInfo":{"status":"ok","timestamp":1719499748065,"user_tz":-330,"elapsed":16403,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"72e75190-a45a-4925-ab9a-2115324e2aea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"]}]},{"cell_type":"code","source":["corpus = ['the sky is blue', 'sky is blue and sky is beautiful', 'the beautiful sky is so blue', 'i love blue cheese']\n","new_doc = ['loving this blue sky today']"],"metadata":{"id":"Y6QdQrVmWLGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"ePh9STCDYtLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BAG OF VECTOR MODEL - BOW\n","def bow_extractor(corpus, ngram_range=(1,1)):\n","  vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n","  features = vectorizer.fit_transform(corpus)\n","  return vectorizer,features"],"metadata":{"id":"QHjFNDIDWPy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build bow vectorizer and get features\n","# ROWS REPRESENT DOCUMENTS COLUMNS REPRESENT WORDS\n","bow_vectorizer,bow_features = bow_extractor(corpus)\n","features = bow_features.todense()\n","print(features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4l-QYFnXCIV","executionInfo":{"status":"ok","timestamp":1719499749673,"user_tz":-330,"elapsed":18,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"b0ea6011-779c-4244-c90a-f52157d15430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 1 0 1 0 1]\n"," [1 1 1 0 2 0 2 0 0]\n"," [0 1 1 0 1 0 1 1 1]\n"," [0 0 1 1 0 1 0 0 0]]\n"]}]},{"cell_type":"code","source":["# extract features from new document using built vectorizer\n","new_doc_features = bow_vectorizer.transform(new_doc)\n","new_doc_features = new_doc_features.todense()\n","print(new_doc_features)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYK7FL56beBC","executionInfo":{"status":"ok","timestamp":1719499749673,"user_tz":-330,"elapsed":13,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"52ce4cb0-6c7c-4f3b-d191-f8154b8659f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 0 0 1 0 0]]\n"]}]},{"cell_type":"code","source":["# print the feature names\n","feature_names = bow_vectorizer.get_feature_names_out()\n","print(feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFwlWNuoWWII","executionInfo":{"status":"ok","timestamp":1719499749673,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"8612c325-cef3-47eb-98f8-6d128ad4a6ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['and' 'beautiful' 'blue' 'cheese' 'is' 'love' 'sky' 'so' 'the']\n"]}]},{"cell_type":"code","source":["# Understanding the feature vectorization with detailed representation\n","import pandas as pd\n","def display_features(features, feature_names):\n","  df = pd.DataFrame(data=features,columns=feature_names)\n","  print(df)\n","display_features(features, feature_names)\n","display_features(new_doc_features, feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhNnPrsZbyZI","executionInfo":{"status":"ok","timestamp":1719499750421,"user_tz":-330,"elapsed":754,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"be368f6b-01d8-4afd-f163-0fd8cc6f0b27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   1     0    1   0    1\n","1    1          1     1       0   2     0    2   0    0\n","2    0          1     1       0   1     0    1   1    1\n","3    0          0     1       1   0     1    0   0    0\n","   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   0     0    1   0    0\n"]}]},{"cell_type":"code","source":["# TF - DF MODEL\n","from sklearn.feature_extraction.text import TfidfTransformer\n","def tfidf_transformer(bow_matrix):\n","    transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n","    tfidf_matrix = transformer.fit_transform(bow_matrix)\n","    return transformer, tfidf_matrix"],"metadata":{"id":"b0cxBN4UdD_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","feature_names = bow_vectorizer.get_feature_names_out()"],"metadata":{"id":"1rv6f147db7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","def tfidf_transformer(bow_matrix):\n","  transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n","  tfidf_matrix = transformer.fit_transform(bow_matrix)\n","  return transformer, tfidf_matrix"],"metadata":{"id":"xf3GetYjcBwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","feature_names = bow_vectorizer.get_feature_names_out()"],"metadata":{"id":"q40f1QJDeMtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build tfidf transformer and show train corpus tfidf features\n","tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n","features = np.round(tdidf_features.todense(), 2)\n","display_features(features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kv1xCX_AeE2Q","executionInfo":{"status":"ok","timestamp":1719499750423,"user_tz":-330,"elapsed":30,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"22460ffa-beb4-4266-f8d4-2dd971cf036f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"code","source":["# show tfidf features for new_doc using built tfidf transformer\n","new_doc_features = np.asarray(new_doc_features)\n","nd_tfidf = tfidf_trans.transform(new_doc_features)\n","nd_features = np.round(nd_tfidf.todense(), 2)\n","display_features(nd_features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpjdiIvCe7ef","executionInfo":{"status":"ok","timestamp":1719499750423,"user_tz":-330,"elapsed":26,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"c9060cf7-9e87-49f2-fe0d-53575de167df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"code","source":["# Understanding how it works in the background\n","import scipy.sparse as sp\n","from numpy.linalg import norm\n","feature_names = bow_vectorizer.get_feature_names_out()\n","# compute term frequency\n","tf = bow_features.todense()\n","tf = np.array(tf, dtype='float64')\n","# show term frequencies\n","display_features(tf, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J751JYmJfgcB","executionInfo":{"status":"ok","timestamp":1719499750424,"user_tz":-330,"elapsed":25,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"45b933a5-0f22-4fb2-d121-f2f5ef8abe1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love  sky   so  the\n","0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n","1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n","2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n","3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"]}]},{"cell_type":"code","source":["# show tfidf features for new_doc using built tfidf transformer\n","nd_tfidf = tfidf_trans.transform(new_doc_features)\n","nd_features = np.round(nd_tfidf.todense(), 2)\n","display_features(nd_features, feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uamug8eGfz5-","executionInfo":{"status":"ok","timestamp":1719499750424,"user_tz":-330,"elapsed":22,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"95ae66a2-a183-459a-a904-e4dcd97d59e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"code","source":["# build the document frequency matrix\n","df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n","df = 1 + df # to smoothen idf later\n","# show document frequencies\n","display_features([df], feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0psl3whf71N","executionInfo":{"status":"ok","timestamp":1719499750425,"user_tz":-330,"elapsed":21,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"08e76c6a-e9e5-4812-caf3-1def92ba52c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    2          3     5       2   4     2    4   2    3\n"]}]},{"cell_type":"code","source":["# compute inverse document frequencies\n","total_docs = 1 + len(corpus)\n","idf = 1.0 + np.log(float(total_docs) / df)\n","# show inverse document frequencies\n","display_features([np.round(idf, 2)], feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBRbwG3Tf-m8","executionInfo":{"status":"ok","timestamp":1719499750425,"user_tz":-330,"elapsed":18,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"f9577ac2-52ab-4daf-e755-3388552514c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"]}]},{"cell_type":"code","source":["# compute idf diagonal matrix\n","total_features = bow_features.shape[1]\n","idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n","idf = idf_diag.todense()\n","# print the idf diagonal matrix\n","print(np.round(idf, 2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3003ujOgKEp","executionInfo":{"status":"ok","timestamp":1719499750425,"user_tz":-330,"elapsed":15,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"e01495c7-f954-40a6-8351-b3b3abd447b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n"," [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n"]}]},{"cell_type":"code","source":["# Compute the tf idf feature matrix using matrix multiplication.\n","tfidf = tf * idf\n","# show tfidf feature matrix\n","display_features(np.round(tfidf, 2), feature_names)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oScmH8x4gRcl","executionInfo":{"status":"ok","timestamp":1719499751028,"user_tz":-330,"elapsed":615,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"bf42a4e2-24e8-42f5-a147-5c4401d29c4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n","1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n","2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n","3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["##Computes the tfidf norms for each document and then divides the tfidf weights with the norm as per the formula to give us the final desired tfidf matrix"],"metadata":{"id":"xz0TzjC0NMPx"}},{"cell_type":"code","source":["# compute L2 norms\n","norms = norm(tfidf, axis=1)\n","# print norms for each document\n","print(np.round(norms, 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkkQG-5oM0ZR","executionInfo":{"status":"ok","timestamp":1719499751029,"user_tz":-330,"elapsed":23,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"ed6b30f0-7032-4476-86f7-1ff3f14294f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2.5  4.35 3.5  2.89]\n"]}]},{"cell_type":"code","source":["# compute normalized tfidf\n","norm_tfidf = tfidf / norms[:, None]\n","# show final tfidf feature matrix\n","display_features(np.round(norm_tfidf, 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT4w7UvtM6FN","executionInfo":{"status":"ok","timestamp":1719499751029,"user_tz":-330,"elapsed":18,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"a40eec48-113f-4b0f-885c-bbe493983470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["### Compare TF-IDF feature matrix for the documents in CORPUS to the feature matrix obtained using TfidfTransformer earlier. They are same, which means the mathematical formula appied to compute is correct."],"metadata":{"id":"26BsTBgIOFRv"}},{"cell_type":"code","source":["# compute the tfidf based feature matrix for new document new_doc.\n","# compute new doc term freqs from bow freqs\n","nd_tf = new_doc_features\n","nd_tf = np.array(nd_tf, dtype='float64')"],"metadata":{"id":"MxMlQvjbOjS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute tfidf using idf matrix from train corpus\n","nd_tfidf = nd_tf*idf\n","nd_norms = norm(nd_tfidf, axis=1)\n","norm_nd_tfidf = nd_tfidf / nd_norms[:, None]"],"metadata":{"id":"o9NcU4OhOwEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show new_doc tfidf feature vector\n","display_features(np.round(norm_nd_tfidf, 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr_dykQMOy-G","executionInfo":{"status":"ok","timestamp":1719499751029,"user_tz":-330,"elapsed":14,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"38fe94cc-d877-4d82-8a25-24887b8528f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"markdown","source":["### Observe again this matrix is same as earlier.\n","### We now implement a generic function that can directly compute the tfidf-based feature vectors for documents from the raw documents themselves.\n"],"metadata":{"id":"1u5iKJDlPYtm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","def tfidf_extractor(corpus, ngram_range=(1,1)):\n","  vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n","  features = vectorizer.fit_transform(corpus)\n","  return vectorizer, features"],"metadata":{"id":"jdNX_C8xP13Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build tfidf vectorizer and get training corpus feature vectors\n","tfidf_vectorizer, tdidf_features = tfidf_extractor(corpus)\n","display_features(np.round(tdidf_features.todense(), 2),feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X71s8EIYQdin","executionInfo":{"status":"ok","timestamp":1719499751030,"user_tz":-330,"elapsed":11,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"b2a743c9-531b-468d-cf15-74fc805b7ce2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"]}]},{"cell_type":"markdown","source":["### Observe that it is again same as above for corpus as well as new_doc"],"metadata":{"id":"PKggReLuQ00-"}},{"cell_type":"code","source":["# get tfidf feature vector for the new document\n","nd_tfidf = tfidf_vectorizer.transform(new_doc)\n","display_features(np.round(nd_tfidf.todense(), 2), feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXLg10TVQxSW","executionInfo":{"status":"ok","timestamp":1719499751030,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"f483d9aa-b72d-47d8-c281-607dd1fcbf38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   and  beautiful  blue  cheese   is  love   sky   so  the\n","0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"]}]},{"cell_type":"markdown","source":["### Modern Word Vector Models - Advanced Word Vectorization Models :"],"metadata":{"id":"sbstAd40DOFG"}},{"cell_type":"code","source":["# Word 2 Vector Model\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kUYbkhZhB7j","executionInfo":{"status":"ok","timestamp":1719499754240,"user_tz":-330,"elapsed":3216,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"98baac1c-676a-4f02-f67d-36354ac6a4b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["import gensim"],"metadata":{"id":"WKe2O2bCgWTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recall\n","corpus = ['the sky is blue', 'sky is blue and sky is beautiful', 'the beautiful sky is so blue', 'i love blue cheese']\n","new_doc = ['loving this blue sky today']"],"metadata":{"id":"jkuc9swcUNOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zfGyzvTFylm","executionInfo":{"status":"ok","timestamp":1719499756314,"user_tz":-330,"elapsed":50,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"944b473d-8f40-4f1e-aec1-c07299f16a21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the sky is blue',\n"," 'sky is blue and sky is beautiful',\n"," 'the beautiful sky is so blue',\n"," 'i love blue cheese']"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["new_doc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIoT7tmZFrOq","executionInfo":{"status":"ok","timestamp":1719499756314,"user_tz":-330,"elapsed":45,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"d2c2b85a-1b20-4448-c569-e0bac8351b57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['loving this blue sky today']"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# tokenize corpora\n","TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in corpus]\n","print(TOKENIZED_CORPUS)\n","tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n","print(tokenized_new_doc)"],"metadata":{"id":"7EDz-JbQhSWY","executionInfo":{"status":"ok","timestamp":1719499756315,"user_tz":-330,"elapsed":43,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c5646f8-9970-41b5-d2de-d8228707dbe1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['the', 'sky', 'is', 'blue'], ['sky', 'is', 'blue', 'and', 'sky', 'is', 'beautiful'], ['the', 'beautiful', 'sky', 'is', 'so', 'blue'], ['i', 'love', 'blue', 'cheese']]\n","[['loving', 'this', 'blue', 'sky', 'today']]\n"]}]},{"cell_type":"code","source":["# build the word2vec model on our training corpus\n","model = gensim.models.Word2Vec(TOKENIZED_CORPUS,vector_size=10,window=10,min_count=2,sample=1e-3)"],"metadata":{"id":"UswehIDiZLiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model creates a vector representation for each word in the vocabulary\n","# Check whether model of Word2Vector type is been created\n","print(type(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Zg0WbTPU4sJ","executionInfo":{"status":"ok","timestamp":1719499756316,"user_tz":-330,"elapsed":39,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"657e630b-6ee3-4bd4-cd1c-23065706379c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'gensim.models.word2vec.Word2Vec'>\n"]}]},{"cell_type":"code","source":["# The Word2Vec model stores word vectors in a separate attribute called wv.\n","# It is a dictionary\n","print(model.wv.key_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmVoWIv-ilUf","executionInfo":{"status":"ok","timestamp":1719499756316,"user_tz":-330,"elapsed":36,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"24583f58-eaab-4da5-9b04-b0c6a0990065"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'blue': 0, 'is': 1, 'sky': 2, 'beautiful': 3, 'the': 4}\n"]}]},{"cell_type":"code","source":["word = \"sky\"\n","vector = model.wv[word]\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17aniugihvIB","executionInfo":{"status":"ok","timestamp":1719499756317,"user_tz":-330,"elapsed":33,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"27b27b3b-70f7-49fa-d277-014ee3bf1c0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n"," -0.00946401  0.05768573 -0.07521638 -0.03936104]\n"]}]},{"cell_type":"code","source":["word = \"blue\"\n","vector = model.wv[word]\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o38hRU-pi0PT","executionInfo":{"status":"ok","timestamp":1719499756317,"user_tz":-330,"elapsed":30,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"a3369b65-4884-45de-d422-ee94539a0694"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n","  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"]}]},{"cell_type":"markdown","source":["### Add all the word vectors and divide the result by the total number of words matched in the vocabulary to get a final resulting averaged word vector representation for the text document."],"metadata":{"id":"tsgVxMPcWqUZ"}},{"cell_type":"code","source":["import numpy as np\n","# define function to average word vectors for a text document\n","def average_word_vectors(words, model, vocabulary, num_features):\n","  feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","  nwords = 0\n","  for word in words:\n","    if word in vocabulary:\n","      nwords = nwords + 1\n","      feature_vector = np.add(feature_vector, model.wv[word])\n","    if nwords:\n","      feature_vector = np.divide(feature_vector,nwords)\n","  return feature_vector"],"metadata":{"id":"1dwcFsofjUbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generalize above function for a corpus of documents\n","def averaged_word_vectorizer(corpus, model, num_features):\n","  vocabulary = set(model.wv.index_to_key)\n","  features = [average_word_vectors(tokenized_sentence, model, vocabulary,num_features)for tokenized_sentence in corpus]\n","  return np.array(features)"],"metadata":{"id":"xzRpjYWSjp7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from gensim.models import Word2Vec"],"metadata":{"id":"OjALL2B9XijG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get averaged word vectors for our training CORPUS\n","avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, model=model, num_features=10)\n","print(np.round(avg_word_vec_features, 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uimT1dRBadu9","executionInfo":{"status":"ok","timestamp":1719499756318,"user_tz":-330,"elapsed":25,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"9ed159eb-777d-401f-a21e-4fe6498158a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.004  0.004  0.008  0.026 -0.025 -0.021  0.015  0.03  -0.021 -0.015]\n"," [-0.009 -0.002  0.015 -0.01  -0.005 -0.004  0.014 -0.009 -0.003 -0.011]\n"," [-0.     0.001  0.01   0.019 -0.019 -0.015  0.013  0.018 -0.011 -0.009]\n"," [-0.005  0.002  0.051  0.09  -0.093 -0.071  0.065  0.09  -0.05  -0.038]]\n"]}]},{"cell_type":"code","source":["# Get averaged word vectors for our training CORPUS\n","avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc, model=model, num_features=10)\n","print(np.round(avg_word_vec_features, 3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNnsvcz0fkoB","executionInfo":{"status":"ok","timestamp":1719499756319,"user_tz":-330,"elapsed":23,"user":{"displayName":"Arya Talekar","userId":"08764980960273992459"}},"outputId":"64e30945-a907-4391-942c-0647e75a7f75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.017  0.013  0.03   0.024 -0.007 -0.026  0.014  0.037 -0.031 -0.019]]\n"]}]},{"cell_type":"markdown","source":["## From the above outputs, we can see that we have uniformly sized averaged word vectors for each document in the corpus, and these feature vectors will be used later for classification by feeding it to the ML algorithms"],"metadata":{"id":"EwUI9piZf-Dc"}}]}